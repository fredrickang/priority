{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import pickle as pk\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset \n",
    "\n",
    "%run autoencoder.ipynb\n",
    "%run Train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../Data/train_dict.pk'\n",
    "val_data_dir = '../Data/test_dict.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "parser.add_argument('--data', default='./data', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('--workers', default=1, type=int, metavar='N', help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=120, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--epoch_decay', default=30, type=int, metavar='N', help='adjust learning rate per N epochs')\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', default=0.001, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--weight_decay', default=0, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print_freq', default=10, type=int, metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--evaluate', default=False,  dest='evaluate', action='store_true', help='evaluate model')\n",
    "parser.add_argument('--pretrained', default=False, dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "parser.add_argument('--seed', default=None, type=int, help='seed for initializing training. ')\n",
    "parser.add_argument('--gpu', default=True, dest='gpu', action='store_true', help='enable GPU.')\n",
    "parser.add_argument('--resnet', default=False, dest='resnet', action='store_true', help='train with resnet18')\n",
    "parser.add_argument('--logging',default= True, help ='logging')\n",
    "parser.add_argument('--num_classes',default = 2, type=int, help='# of classes')\n",
    "\n",
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        fo = open(data_dir,'rb')\n",
    "        self.data_info = pk.load(fo)\n",
    "        fo.close()\n",
    "        self.data_arr = self.data_info['data']\n",
    "        self.label_arr = self.data_info['label']\n",
    "        \n",
    "        self.data_len = len(self.data_info['label'])\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        single_data_name = self.data_arr[index]\n",
    "        \n",
    "        data_as_tensor = self.to_tensor(single_data_name)\n",
    "        single_data_label = self.label_arr[index]\n",
    "        \n",
    "        return (data_as_tensor, single_data_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, best_acc1\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    # create model\n",
    "    model = autoencoder()\n",
    "    if args.gpu:\n",
    "        print(\"=> gpu in now working\")\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr, betas=(0.9, 0.999))\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc1 = checkpoint['best_acc1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    my_train_dataset = CustomDataset(train_data_dir)\n",
    "    my_val_dataset = CustomDataset(val_data_dir)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = my_train_dataset, batch_size = 128, shuffle =True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset= my_val_dataset, batch_size = 128, shuffle= False)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion, args.gpu, args.print_freq, f)\n",
    "        return\n",
    "    \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, args.lr, epoch, args.epoch_decay)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, args.gpu, args.print_freq)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1 = validate(val_loader, model, criterion, args.gpu, args.print_freq)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, model = 'autoencoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data DIR] [--workers N] [--epochs N]\n",
      "                             [--epoch_decay N] [--start_epoch N]\n",
      "                             [--batch_size N] [--lr LR] [--weight_decay W]\n",
      "                             [--print_freq N] [--resume PATH] [--evaluate]\n",
      "                             [--pretrained] [--seed SEED] [--gpu] [--resnet]\n",
      "                             [--logging LOGGING] [--num_classes NUM_CLASSES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/fredrickang/Library/Jupyter/runtime/kernel-9a341abf-7d25-4c8a-9c1d-e5ab548dd1fb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
